{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **FastText in NLP**\n",
    "\n",
    "**FastText** is a word embedding technique developed by Facebook's AI Research (FAIR) that extends the Word2Vec model by incorporating subword information. Instead of treating each word as a distinct unit, FastText breaks words down into character n-grams (subword units). This allows it to better handle **morphologically rich languages** and **out-of-vocabulary (OOV)** words.\n",
    "\n",
    "#### Key Concepts:\n",
    "- **Subword Information**: FastText learns embeddings for character n-grams. For example, the word *\"where\"* could be broken into n-grams like *\"wh\"*, *\"her\"*, and *\"re\"*. The final word vector is the sum of its n-gram vectors.\n",
    "- **OOV Words**: Since FastText uses subwords, it can generate embeddings for words not seen during training (unlike Word2Vec).\n",
    "  \n",
    "#### Example:\n",
    "For the word **\"apple\"**, FastText would generate embeddings not just for the word itself, but also for its n-grams, like:\n",
    "- `\"app\"`, `\"ple\"`, `\"appl\"`, etc.\n",
    "\n",
    "The final word vector for **\"apple\"** is the sum of these subword vectors.\n",
    "\n",
    "#### Advantages:\n",
    "- Works better with rare words or words that share similar roots.\n",
    "- Useful for languages with complex morphology, like German or Finnish.\n",
    "\n",
    "FastText is commonly used for tasks like text classification, language modeling, and machine translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import fasttext\n",
    "import gzip\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "\n",
    "with gzip.open('cc.en.300.bin.gz', 'rb') as f_in:\n",
    "    with open('cc.en.300.bin', 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 4.47 GiB for an array with shape (1200000000,) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m fModel\u001b[38;5;241m=\u001b[39m\u001b[43mfasttext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_facebook_vectors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcc.en.300.bin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Projects\\NLP\\nlp_venv\\Lib\\site-packages\\gensim\\models\\fasttext.py:784\u001b[0m, in \u001b[0;36mload_facebook_vectors\u001b[1;34m(path, encoding)\u001b[0m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_facebook_vectors\u001b[39m(path, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    732\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load word embeddings from a model saved in Facebook's native fasttext `.bin` format.\u001b[39;00m\n\u001b[0;32m    733\u001b[0m \n\u001b[0;32m    734\u001b[0m \u001b[38;5;124;03m    Notes\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    782\u001b[0m \n\u001b[0;32m    783\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 784\u001b[0m     full_model \u001b[38;5;241m=\u001b[39m \u001b[43m_load_fasttext_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m full_model\u001b[38;5;241m.\u001b[39mwv\n",
      "File \u001b[1;32md:\\Projects\\NLP\\nlp_venv\\Lib\\site-packages\\gensim\\models\\fasttext.py:808\u001b[0m, in \u001b[0;36m_load_fasttext_format\u001b[1;34m(model_file, encoding, full_model)\u001b[0m\n\u001b[0;32m    789\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load the input-hidden weight matrix from Facebook's native fasttext `.bin` output files.\u001b[39;00m\n\u001b[0;32m    790\u001b[0m \n\u001b[0;32m    791\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    805\u001b[0m \n\u001b[0;32m    806\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    807\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mopen(model_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[1;32m--> 808\u001b[0m     m \u001b[38;5;241m=\u001b[39m \u001b[43mgensim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fasttext_bin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    810\u001b[0m model \u001b[38;5;241m=\u001b[39m FastText(\n\u001b[0;32m    811\u001b[0m     vector_size\u001b[38;5;241m=\u001b[39mm\u001b[38;5;241m.\u001b[39mdim,\n\u001b[0;32m    812\u001b[0m     window\u001b[38;5;241m=\u001b[39mm\u001b[38;5;241m.\u001b[39mws,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    821\u001b[0m     max_n\u001b[38;5;241m=\u001b[39mm\u001b[38;5;241m.\u001b[39mmaxn,\n\u001b[0;32m    822\u001b[0m )\n\u001b[0;32m    823\u001b[0m model\u001b[38;5;241m.\u001b[39mcorpus_total_words \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mntokens\n",
      "File \u001b[1;32md:\\Projects\\NLP\\nlp_venv\\Lib\\site-packages\\gensim\\models\\_fasttext_bin.py:348\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fin, encoding, full_model)\u001b[0m\n\u001b[0;32m    345\u001b[0m raw_vocab, vocab_size, nwords, ntokens \u001b[38;5;241m=\u001b[39m _load_vocab(fin, new_format, encoding\u001b[38;5;241m=\u001b[39mencoding)\n\u001b[0;32m    346\u001b[0m model\u001b[38;5;241m.\u001b[39mupdate(raw_vocab\u001b[38;5;241m=\u001b[39mraw_vocab, vocab_size\u001b[38;5;241m=\u001b[39mvocab_size, nwords\u001b[38;5;241m=\u001b[39mnwords, ntokens\u001b[38;5;241m=\u001b[39mntokens)\n\u001b[1;32m--> 348\u001b[0m vectors_ngrams \u001b[38;5;241m=\u001b[39m \u001b[43m_load_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m full_model:\n\u001b[0;32m    351\u001b[0m     hidden_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Projects\\NLP\\nlp_venv\\Lib\\site-packages\\gensim\\models\\_fasttext_bin.py:282\u001b[0m, in \u001b[0;36m_load_matrix\u001b[1;34m(fin, new_format)\u001b[0m\n\u001b[0;32m    280\u001b[0m     matrix \u001b[38;5;241m=\u001b[39m _fromfile(fin, _FLOAT_DTYPE, count)\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 282\u001b[0m     matrix \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_FLOAT_DTYPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m matrix\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (count,), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexpected (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m,),  got \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (count, matrix\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    285\u001b[0m matrix \u001b[38;5;241m=\u001b[39m matrix\u001b[38;5;241m.\u001b[39mreshape((num_vectors, dim))\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 4.47 GiB for an array with shape (1200000000,) and data type float32"
     ]
    }
   ],
   "source": [
    "fModel=fasttext.load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
